{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3zfZkmeImfC"
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu openai sentence-transformers langdetect PyMuPDF\n",
    "!pip install -q nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2ZRCDdsKNO6",
    "outputId": "ffa759cb-453d-4587-dca5-ba44307607f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvuK2Kb-KQ58"
   },
   "outputs": [],
   "source": [
    "# üìå Replace this path with your own\n",
    "pdf_path = \"/content/drive/MyDrive/Colab Notebooks/HSC26-Bangla1st-Paper.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xK211P-cMnMZ",
    "outputId": "022d8ea7-c014-472c-e50c-1305ff3dc4d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import faiss\n",
    "import openai\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import Markdown\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ====== Set your OpenAI API Key here ======\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# ====== Config ======\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "CHUNK_SIZE = 2\n",
    "TOP_K = 3\n",
    "\n",
    "model = SentenceTransformer(EMBED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZX2S46HNB6S",
    "outputId": "3dd89eda-a1c3-4927-f187-ea044e74f6a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Force-remove corrupted/cached nltk_data if it exists\n",
    "shutil.rmtree(\"/root/nltk_data\", ignore_errors=True)\n",
    "\n",
    "# Ensure a clean download\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5UFMEDLNVHC"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    import fitz\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def bangla_chunk_text(text, size=2):\n",
    "    # Split by Bangla sentence delimiter\n",
    "    sentences = text.split(\"‡•§\")\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = ['‡•§ '.join(sentences[i:i+size]) + \"‡•§\" for i in range(0, len(sentences), size)]\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQbnhp1RNgjh",
    "outputId": "bacc82f8-7db9-411a-ad9a-b045fe954a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 481 chunks.\n"
     ]
    }
   ],
   "source": [
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "chunks = bangla_chunk_text(raw_text, size=2)\n",
    "print(f\"‚úÖ Extracted {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a3e5d2c54c3b4adab4fd7b141dd523ea",
      "7ea839619e264c4ea657f17a4c3b43b8",
      "386c1cb96d384ff286fba79a160ba040",
      "03676e04833c41f0940eaa6264cdfde0",
      "1ff2b784b7c34253936442d17e60b8b1",
      "510d83bc20ea408b9b0ad84ccda0b1cc",
      "2e0bf3fb1853480aba513c690edc6f5c",
      "48cff79a643043e9a2713e41e1d43f1e",
      "55427612ef1f43eda0368eba106a42b5",
      "fb007d49fe5d4799aa65fd506f51566d",
      "af1ccb59538e45b19eff55143214885f"
     ]
    },
    "id": "ZWhlxeJFNi7Z",
    "outputId": "7be17856-615a-4f98-aa77-3358e391640d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e5d2c54c3b4adab4fd7b141dd523ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector index created.\n"
     ]
    }
   ],
   "source": [
    "def embed_chunks(chunks):\n",
    "    return model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "chunk_embeddings = embed_chunks(chunks)\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "index.add(np.array(chunk_embeddings).astype('float32'))\n",
    "print(\"‚úÖ Vector index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBQdGJzbNqk4"
   },
   "outputs": [],
   "source": [
    "def search(query, k=TOP_K):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding).astype('float32'), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"\"\"Answer the following question based on the context below.\n",
    "Respond in the same language as the query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def answer_query(query):\n",
    "    context = \"\\n\".join(search(query))\n",
    "    answer = generate_answer(query, context)\n",
    "    display(Markdown(f\"**Q:** {query}\\n\\n**A:** {answer}\"))\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0JVcJv2Nt7B"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "answer_query(\"‡¶Ö‡¶®‡¶™‡ßÅ‡ßá‡¶Æ‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?\")\n",
    "\n",
    "# Example 2\n",
    "answer_query(\"‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡¶™‡ßÅ‡ßá‡¶Æ‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?\")\n",
    "\n",
    "# Example 3\n",
    "answer_query(\"‡¶¨‡¶ø‡ßü‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡ßü‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Tx6sz--N5vB"
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.28 --upgrade --quiet\n",
    "import openai\n",
    "openai.api_key = \"your-openai-api-key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGlNkLP8T-Fg",
    "outputId": "393c95dd-cb4f-4fc5-ae99-efdc515d847f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 0.28.0\n",
      "Uninstalling openai-0.28.0:\n",
      "  Successfully uninstalled openai-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install -q openai==0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvntFus3T_yH"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"  # üîê Set your key here again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7UPzvLGUrLR"
   },
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    prompt = f\"\"\"Answer the following question based on the context below.\n",
    "Respond in the same language as the query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "658ea1847d944513930d908f98f5e6e7",
      "23dd79131ce9479bbfc4e7ecef0bd734",
      "54366b7f21fb403ba907bea76114f449",
      "3ba5c3371ae84a709badf1ebea61dd9b",
      "7f447e89a0804fb4a78c993581fb6c8c",
      "00163fd63c5a41d2b424409a4195f4b7",
      "1e994158233c47b29a2ada56622deecb",
      "9c9bbfcc60d844579fd155a92b4a7323",
      "07450937e15c4bb9bc36c1b104a6f433",
      "1d2aab140b44408783597a76b86d1c73",
      "24317989808e4ceebb35617fd52805fa"
     ]
    },
    "id": "uyLV67LhVI_u",
    "outputId": "8c99c51e-c709-4f31-d2a0-a76cc8755b57"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658ea1847d944513930d908f98f5e6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings).astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wxH3KkwVK5W"
   },
   "outputs": [],
   "source": [
    "def search(query, k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding).astype('float32'), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def generate_answer_v1(query, context):\n",
    "    prompt = f\"\"\"Answer the following question using the context below.\n",
    "Respond in the same language as the query.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FPYDH6dWRwX",
    "outputId": "e564a992-a99e-42ce-9652-b65a51f1bbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.1.1\n",
      "  Downloading openai-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (2.11.7)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.11/dist-packages (from openai==1.1.1) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.5.0->openai==1.1.1) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.5.0->openai==1.1.1) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.1.1) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.1.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.1.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.1.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.1.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.1.1) (0.4.1)\n",
      "Downloading openai-1.1.1-py3-none-any.whl (217 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "Successfully installed openai-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir openai==1.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH3GiyqmWS7P",
    "outputId": "ca36b077-9fc2-4daa-f8ee-e53759d7d2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.1.1\n",
      "Uninstalling openai-1.1.1:\n",
      "  Successfully uninstalled openai-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlOt41iyXD6O"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai_client = openai.OpenAI(api_key=\"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\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
